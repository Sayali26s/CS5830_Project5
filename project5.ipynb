{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of the dataset\n",
    "A_id - Unique identifier for each fruit \\\n",
    "Size - Size of the fruit \\\n",
    "Weight - Weight of the fruit \\\n",
    "Sweetness - Sweetness of the fruit \\\n",
    "Crunchiness - Texture indicating the crunchiness of the fruit \\\n",
    "Juiciness - Level of juiciness of the fruit \\\n",
    "Ripeness - Stage of ripeness of the fruit \\\n",
    "Acidity - Acidity level of the fruit \\\n",
    "Quality - Overall quality of the fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Load the dataset\n",
    "apple = pd.read_csv(\"data/apple_quality.csv\")\n",
    "apple[\"Quality\"] = apple[\"Quality\"].replace([\"bad\"], 0)\n",
    "apple[\"Quality\"] = apple[\"Quality\"].replace([\"good\"], 1)\n",
    "apple=apple[['Size', 'Weight','Sweetness', 'Crunchiness','Juiciness', 'Ripeness', 'Acidity','Quality']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"darkred\", \"red\"]\n",
    "\n",
    "# Create the count plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "ax = sns.countplot(data=apple, x='Quality', hue= 'Quality', palette=colors)\n",
    "\n",
    "# Add count annotations to each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.title(\"Quality Distribution\")\n",
    "plt.xlabel(\"Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"darkred\", \"red\"]\n",
    "\n",
    "# Create the subplots\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(apple.columns, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    plt.title(f\"Distribution of {col} Data\")\n",
    "    sns.histplot(data=apple, x=col, kde=True, hue=\"Quality\", palette=colors)\n",
    "    # print(sns.histplot(data=apple, x=col, kde=True, hue=\"Quality\", palette=colors))\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Size', 'Weight', 'Sweetness', 'Crunchiness', 'Juiciness', 'Ripeness', 'Acidity']\n",
    "\n",
    "apple = apple.drop(4000) # dropping this row because it contains the author info (messes with our data)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = apple[features].corr()\n",
    "\n",
    "# Plot correlation matrix as heatmap\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "# plt.title(\"Correlation Matrix\")\n",
    "# plt.show()\n",
    "\n",
    "# Plot scatter plots with regression lines\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.scatter(x=apple[features[i]], y=apple[features[j]], color='red')\n",
    "        plt.xlabel(features[i])\n",
    "        plt.ylabel(features[j])\n",
    "        plt.title(f\"{features[i]} vs {features[j]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_df = apple.corr(method='pearson')\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "mask = np.triu(np.ones_like(correlation_matrix_df, dtype=bool))\n",
    "sns.heatmap(correlation_matrix_df,annot=True,cmap='Reds', mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(apple,hue='Quality',palette='Reds',corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "Size, sweetness and juiciness are relatively, strongly positively correlated with the good quality. The correlation may seem to be low due to the binary value of good quality\\\n",
    "Ripeness is relatively, strongly negatively correlated with the good quality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Gaussian for datasets with quantitative variables\n",
    "clf = GaussianNB()\n",
    "\n",
    "apple = apple.dropna()\n",
    "X_train, X_test, y_train, y_test = train_test_split(apple[['Size', 'Weight','Sweetness', 'Crunchiness',\n",
    "                                                           'Juiciness', 'Ripeness', 'Acidity']].values,\n",
    "                                                    apple.Quality,test_size=0.25)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=[1,0])\n",
    "\n",
    "print(classification_report(y_true=y_test,y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgnb = confusion_matrix(y_test, y_pred)\n",
    "mgnb\n",
    "cm_columns = ['predicted 0', 'predicted 1']\n",
    "cm_rows = ['actual 0', 'actual 1']\n",
    "df_nb = pd.DataFrame(mgnb, index = cm_columns, columns = cm_rows)\n",
    "sns.heatmap(df_nb, annot=True, fmt='d',cmap='Reds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using KNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors = 5)\n",
    "model = model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=[1,0])\n",
    "print(p, r, f, s)\n",
    "print(classification_report(y_true=y_test,y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgnb = confusion_matrix(y_test, y_pred)\n",
    "mgnb\n",
    "cm_columns = ['predicted 0', 'predicted 1']\n",
    "cm_rows = ['actual 0', 'actual 1']\n",
    "df_nb = pd.DataFrame(mgnb, index = cm_columns, columns = cm_rows)\n",
    "sns.heatmap(df_nb, annot=True, fmt='d',cmap='Reds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out which attribute does worst (on average) in terms of precision with Naive Bayes Classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['Size', 'Weight','Sweetness', 'Crunchiness', 'Juiciness', 'Ripeness', 'Acidity']\n",
    "precision_scores = {}\n",
    "times_to_sample = 10\n",
    "\n",
    "# How well does predicting with a single attribute perform?\n",
    "for attr in range(len(attributes)):\n",
    "    scores = []\n",
    "    for j in range(times_to_sample):\n",
    "        classifier = GaussianNB()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(apple[[attributes[attr]]].values, apple.Quality, test_size=0.25)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=[0,1])\n",
    "        scores.append(p)\n",
    "    precision_scores[attributes[attr]] = scores\n",
    "\n",
    "precision_averages_label_zero = []\n",
    "precision_averages_label_one = []\n",
    "\n",
    "for attr in range(len(attributes)):\n",
    "    total_label_zero = 0\n",
    "    total_label_one = 0\n",
    "    for j in range(times_to_sample):\n",
    "        total_label_zero += precision_scores[attributes[attr]][j][0]\n",
    "        total_label_one += precision_scores[attributes[attr]][j][1]\n",
    "\n",
    "    precision_averages_label_zero.append(total_label_zero / times_to_sample)\n",
    "    precision_averages_label_one.append(total_label_one / times_to_sample)\n",
    "\n",
    "min_index_label_zero = min(enumerate(precision_averages_label_zero), key=lambda x: x[1])[0]\n",
    "min_index_label_one = min(enumerate(precision_averages_label_one), key=lambda x: x[1])[0]\n",
    "\n",
    "print(f\"The worst precision for label zero was: {attributes[min_index_label_zero]}\")\n",
    "print(f\"The worst precision for label one was: {attributes[min_index_label_one]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_lists_label_zero = [precision_scores[attr][0] for attr in attributes]\n",
    "precision_lists_label_one = [precision_scores[attr][1] for attr in attributes]\n",
    "\n",
    "# Create boxplots for precision scores of label zero and label one\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(precision_lists_label_zero, labels=attributes, boxprops=dict(color='red'))\n",
    "plt.title('Precision Scores for Label Zero by Attribute')\n",
    "plt.xlabel('Attribute')\n",
    "plt.ylabel('Precision')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(precision_lists_label_one, labels=attributes, boxprops=dict(color='red'))\n",
    "plt.title('Precision Scores for Label One by Attribute')\n",
    "plt.xlabel('Attribute')\n",
    "plt.ylabel('Precision')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acidity was typically the worst when used with the Naive Bayes Classifier. Does our precision increase if we predict without acidity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try with a few (but not all) attributes\n",
    "clf = GaussianNB()\n",
    "\n",
    "# with acidity (using random_state = 42 for consistency between samples)\n",
    "X_train, X_test, y_train, y_test = train_test_split(apple[['Size', 'Weight','Sweetness', 'Crunchiness', 'Juiciness', 'Ripeness']].values,\n",
    "                                                    apple.Quality,test_size=0.25, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=[1,0])\n",
    "print(f\"With Acidity \\t | Precision: {p} Recall: {r} F-Score: {f} Support: {s}\\n\")\n",
    "\n",
    "# without acidity (using random_state = 42 for consistency between samples)\n",
    "X_train, X_test, y_train, y_test = train_test_split(apple[['Size', 'Weight','Sweetness', 'Crunchiness', 'Juiciness', 'Ripeness', 'Acidity']].values,\n",
    "                                                    apple.Quality,test_size=0.25, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=[1,0])\n",
    "print(f\"W/out Acidity \\t | Precision: {p} Recall: {r} F-Score: {f} Support: {s}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Ripeness correspond to other attributes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax = sns.scatterplot(data=apple.sample(frac=0.05), x=\"Ripeness\", y=\"Sweetness\",color='red')\n",
    "sns.regplot(data=apple.sample(frac=0.25), x='Ripeness', y='Sweetness',color='red')\n",
    "plt.show()\n",
    "\n",
    "# correlation coefficients of ripeness compared to other attributes\n",
    "display(stats.pearsonr(apple.dropna().Ripeness, apple.dropna().Sweetness))\n",
    "display(stats.pearsonr(apple.dropna().Ripeness, apple.dropna().Juiciness))\n",
    "display(stats.pearsonr(apple.dropna().Ripeness, apple.dropna().Crunchiness))\n",
    "display(stats.pearsonr(apple.dropna().Ripeness, apple.dropna().Size))\n",
    "display(stats.pearsonr(apple.dropna().Ripeness, apple.dropna().Weight))\n",
    "display(stats.pearsonr(apple.dropna().Ripeness, apple.dropna().Acidity.astype(float)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad apples vs. good apples by above-average values for each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large = apple[apple.Weight > 0]\n",
    "large = large[large.Size >0]\n",
    "sweet_apples = apple[apple.Sweetness>0]\n",
    "crunchy_apples = apple[apple.Crunchiness>0]\n",
    "juicy_apples = apple[apple.Juiciness>0]\n",
    "ripe_apples = apple[apple.Ripeness>0]\n",
    "acidic_apples = apple[apple.Acidity.astype('float') >0]\n",
    "\n",
    "sns.countplot(x='Quality', data=large, hue='Quality', palette='Reds')\n",
    "plt.title('Count of Bad vs. Good Apples (Large/Heavy)')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='Quality', data=sweet_apples, hue='Quality', palette='Reds')\n",
    "plt.title('Count of Bad vs. Good Apples (Sweet Apples)')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='Quality', data=crunchy_apples, hue='Quality', palette='Reds')\n",
    "plt.title('Count of Bad vs. Good Apples (Crunchy Apples)')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='Quality', data=juicy_apples, hue='Quality', palette='Reds')\n",
    "plt.title('Count of Bad vs. Good Apples (Juicy Apples)')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='Quality', data=ripe_apples, hue='Quality', palette='Reds')\n",
    "plt.title('Count of Bad vs. Good Apples (Ripe Apples)')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='Quality', data=acidic_apples, hue='Quality', palette='Reds')\n",
    "plt.title('Count of Bad vs. Good Apples (Acidic Apples)')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
